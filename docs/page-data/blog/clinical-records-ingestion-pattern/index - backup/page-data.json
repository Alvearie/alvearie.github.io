{"componentChunkName":"component---src-pages-blog-clinical-records-ingestion-pattern-index-backup-mdx","path":"/blog/clinical-records-ingestion-pattern/index - backup/","result":{"pageContext":{"frontmatter":{"title":"Using Open Source to Build a Healthcare Record Ingestion Pattern","description":"Clinical Ingestion Blog"},"relativePagePath":"/blog/clinical-records-ingestion-pattern/index - backup.mdx","titleType":"page","MdxNode":{"id":"4d3d4c01-b926-5c1d-a8c0-5ec8a958b4ab","children":[],"parent":"15acc5ab-c4f5-53ad-a1d1-d0928dfe1ccb","internal":{"content":"---\r\ntitle: Using Open Source to Build a Healthcare Record Ingestion Pattern\r\ndescription: Clinical Ingestion Blog\r\n---\r\nBy Luis A. Garcia &nbsp;&nbsp; | &nbsp;&nbsp; Published February 9, 2021\r\n\r\n### Introduction\r\n\r\nThe increasing digitization of healthcare records has made it more important to have a set of technologies and infrastructure \r\nthat enable healthcare organizations to effectively create, store, transform, exchange, and consume these records. This post \r\ndemonstrates how it is possible to build a reference implementation for processing healthcare records that addresses several \r\ncommon use cases, using only open source technologies.\r\n\r\nThis post introduces the Alvearie Clinical Ingestion Pattern, a reference implementation to ingest, process and store clinical records \r\nusing existing healthcare record standards. The framework fulfills several design considerations that will be outlined in the \r\nnext section and that can be extended to implement typical healthcare record use cases. It is built using open source technologies \r\nand a Helm Chart is provided in order to facilitate its deployment on a Kubernetes cluster, running potentially on any public or \r\nprivate cloud.\r\n\r\n### Design Considerations\r\n\r\nAs healthcare records have become more digitized, there have been several steps made in the right direction to facilitate \r\nworking with those records and some fundamental problems have already been solved. For example, the previously disparate \r\nset of data representation formats used by the various healthcare organizations made it harder to interoperate, however \r\nmore organizations agree on a standard way to represent and exchange these healthcare records as a result of [Health \r\nLevel Seven International (HL7)](https://www.hl7.org) creating the [Fast Healthcare Interoperability Resources (FHIR)](https://www.hl7.org/implement/standards/product_brief.cfm?product_id=491)\r\n specification. While agreeing on a standard for exchanging data is crucial to creating a healthcare records processing \r\n pipeline, it is only step one. In this section we try to discuss and solve some of additional aspects needed in order\r\n to process healthcare records effectively.\r\n \r\nThere are several non-functional characteristics that need to be considered in order to build a true healthcare records \r\nprocessing pattern that will be able to meet the challenges of the modern healthcare organization. Those characteristics include:\r\n  -\tCloud based: It should be a native cloud application with all the benefits that entails. \r\n  -\tExtensible: It should be possible for users to extend it to accomplish additional use cases on top of the basic functionality provided by the pattern. \r\n  -\tFlexible: It should be possible for users to modify comprising elements of the pattern and replace them with elements deemed better suited for the users' purposes, i.e. a \"bring your own\" (BYO) service model. \r\n  -\tOpen: It should not create vendor lock-in on any given cloud or technology, in other words it should be a multi-cloud application.\r\n  -\tScalable: It should scale as necessary to meet the user's data performance and throughput needs.\r\n\r\nFrom a functional perspective, a healthcare records processing pattern needs to allow its users to input healthcare records of \r\nmultiple kinds using some of the more commonly used formats, e.g. FHIR v4 and HL7 v3. The mechanism to accept records should \r\nallow for high throughput and the records should be persisted for traceability purposes. Multiple input modalities may be useful, \r\nfor instance input via a messaging framework and over HTTP. The records should flow through an ingestion pipeline that would \r\nnormalize the data, validate it, potentially transform it and ultimately store it. Appropriate logging, resiliency, error \r\nreporting and metrics should be maintained throughout the process.\r\n\r\nIn general, processing healthcare records falls within the realm of what is known as data integration, which is the process of \r\ncombining data from different sources and providing users a unified view of said data. Data integration use cases may involve \r\nnon-engineering teams, therefore it is a design consideration of the pattern covered in this post to provide non-engineering \r\nteams with a simple way to extend and modify the data flows.\r\n \r\n### Clinical Data Ingestion Pattern\r\n\r\nThe entire pattern is built using open source technologies, and if needed it could be expanded in functionality and scope \r\nusing other components, proprietary or open source. The following open source technologies are used in the framework:\r\n  - [Apache NiFi](https://nifi.apache.org/) is a platform for automating and managing the flow of data between disparate systems. \r\n  - [Apache NiFi Registry](https://nifi.apache.org/registry.html) is a complementary application that provides a central location for storage and management of shared resources across one or more instances of Apache NiFi. \r\n  - [Apache Kafka](https://kafka.apache.org/) is a distributed streaming platform for publishing, subscribing, storing and processing streams of records.\r\n  - [IBM FHIR Server](https://github.com/IBM/FHIR) is a modular Java implementation of version 4 of the HL7 FHIR specification with a focus on performance and configurability.\r\n  - [Prometheus](https://prometheus.io/) is an open source monitoring and alerting tool that is widely adopted across many enterprises, that monitors targets by scraping or pulling metrics from endpoints and stores the metrics in a time series database.\r\n  - [Grafana](https://grafana.com/) is an open source tool for data visualization and monitoring. Data sources such as Prometheus can be added to Grafana for metrics collection. It includes powerful visualization capabilities for graphs, tables, and heatmaps. \r\n\r\nThe foundation of the pattern is **Apache NiFi**. Apache NiFi provides a graphical user interface in the form of a canvas for data \r\nintegrators to build data processing workflows using what are known as \"processors\". NiFi comes out of the box with multiple \r\nprocessors that allow for data input and output to and from various sources, and for transforming the data into multiple data \r\nformats. Processors are linked together in such a way that the output of one becomes the input to the other, and it is in this \r\nmanner that processor groups of increasing complexity can be built.\r\n\r\nOnce a sufficiently complex and functional group of processors that achieves a specific purpose is built, it can typically be \r\nmanaged independently. Think of it as how a class, a method or a function can be abstracted out in code and managed independently \r\nas a utility. These modular process groups in NiFi are stored and managed in a **NiFi Registry**. The NiFi Registry allows for \r\nversion control and sharing of NiFi process groups, and it is backed by a Github repository where a team can be continuously \r\ndelivering process group updates or new process groups to be consumed by a NiFi server user. The NiFi Registry is the second \r\nelement of this pattern, and it is used to deliver the various NiFi processors used by the ingestion pattern. You can think \r\nof the NiFi canvas and NiFi Registry respectively as a big user-friendly empty LEGO&copy; board and a bag of working LEGO&copy; pieces \r\nthat you will use to build structures on your board.\r\n\r\nNow, in order to get data flowing in and out of the data pipeline we need a mechanism that allows for processing of the \r\ndata flowing through the pipeline in a way that is: \r\n\r\n  - Continuous, such that users don't need to wait for the pipeline to finish processing some previous data before they can submit new data \r\n  - Asynchronous, such that users don't need to wait synchronously for data to be fully processed since processing of data will likely take time\r\n  - Reliable, such that users know that once the data has been accepted it will be processed and not lost\r\n  - Durable, such that users can keep track of the raw data they have submitted into the pipeline for traceability purposes as long as necessary. \r\n\r\n**Apache Kafka** is an open source events stream framework that addresses these characteristics. The ingestion pattern connects to \r\na Kafka topic, which is just a message queue, listening for clinical data posted there, so that when data appears, it gets \r\npicked up by the framework and ingested. While posting to a Kafka topic is the main data input method, the Ingestion \r\nPattern also accepts data via a conventional REST API.\r\n\r\nClinical data that is posted to the Ingestion Pattern typically needs to be processed in various ways. Processing may include:\r\n  - Data Validation: Ensuring that the clinical data being ingested adheres to some conventions, standards or schemas. For instance, ensure the patient clinical data being ingested adheres to the FHIR US Core Profile.\r\n  - Data Transformation: Transforming the clinical data into a different format or fixing any validation errors by ensuring the failing fields adhere to the expected schemas. For instance, transforming clinical data in HL7 v3 format to FHIR, or adjusting a field to match some desired FHIR profile.\r\n  - Data Enrichment: Processing the incoming clinical data with the purpose of enriching it with more data, or metadata, that will be useful for some consumers. For instance, running some natural language processor over patient clinical notes to try to discover new clinical information.\r\n  - Data De-Identification: A very specific form of data transformation that removes any personally identifiable elements from clinical records.\r\n\r\nThe Ingestion Pattern includes pre-defined spots where one or more of these processing steps can be plugged in. It also includes \r\na configuration mechanism to specify, on a per record basis, exactly which operations from the available processing steps to \r\nrun. The pattern includes default implementations for some of these processors, but those can be replaced or complemented with \r\nothers that can be plugged in to satisfy different user needs.\r\n\r\nAfter the clinical data has been processed, it may need to be persisted. The standard for persisting clinical data is using a \r\nFHIR server. The Ingestion Pattern includes the **IBM FHIR Server** setup out of the box, which serves as the default target for \r\ningested data. That FHIR server is exposed outside the pattern where clinical data consumers can access it using the FHIR Server \r\nREST APIs.\r\n\r\nThe Ingestion Pattern includes some default activity monitoring for records that have been put through via **Prometheus** monitors \r\nover its existing components, as well as processor and memory usage of some of its components. It also includes monitors over the \r\nactual Kubernetes cluster where the pattern is running. All monitoring information can be visualized using the included **Grafana** \r\ninstance.\r\n \r\n### Installation\r\n\r\nThe instructions here assume that you have a working Kubernetes Cluster 1.10+ with Helm 3.0+.\r\n\r\nThe multiple components of the Ingestion Pattern can be deployed in a single step on a **Kubernetes** Cluster using the\r\n Alvearie Ingestion **Helm** chart. The chart performs all the steps necessary to deploy the Ingestion Pattern, ensure that \r\n there is connectivity between its various elements, set up the NiFi canvas with the corresponding processors, and initialize \r\n the necessary components. This greatly simplifies the startup process.\r\n \r\nThe following simple steps are necessary in order to run the Alvearie Clinical Ingestion Pattern:\r\n1.\tCheck out the code<br/>\r\n\t`git clone https://github.com/Alvearie/health-patterns.git`<br/>\r\n\t`cd clinical-ingestion/helm-charts/alvearie-ingestion`\r\n\t\r\n2.\tOptionally create a new namespace in your Kubernetes Cluster. <br/>\r\n\tIt is recommended, though not required, that you create a namespace before installing the chart in order to prevent the various \r\n\tartifacts it will install from mixing with the rest of the artifacts in your Kubernetes cluster, in an effort to make management \r\n\teasier.<br/>\r\n\t`kubectl create namespace alvearie`<br/>\r\n\t`kubectl config set-context --current --namespace=alvearie`\r\n3.\tInstall the helm chart with a release name ingestion:<br/>\r\n\t`helm install ingestion .`\r\n\r\nAfter running the commands above, you will see that all the corresponding elements of the Ingestion Pattern will start to be \r\nlaid out, and eventually all the Kubernetes resources will be up and running:\r\n \r\n![Screen Shot](BlogSS001.png)\r\n\r\nArchitecturally, that helm install has deployed the following:\r\n\r\n![Ingestion Architecture](BlogArch.png)\r\n \r\n### Using the Ingestion Pattern\r\n \r\nBy default, there are four external services exposed by the Alvearie Clinical Ingestion Pattern: \r\nNiFi, Kafka, FHIR, and Grafana. Let's go through them one by one along with their corresponding functionality. \r\n\r\nTo obtain the external IP for each of the exposed services run the following command:\r\n\r\n   `kubectl get services | grep LoadBalancer`\r\n\r\nNote the result of this command also includes the corresponding ports where the services are available, but the default \r\nports are also specified here for simplicity.\r\n\r\nLet's start with the *ingestion-nifi* service: [http://{nifi-external-ip}:8080/nifi](http://{nifi-external-ip}:8080/nifi)\r\n\r\n![Nifi Screen Shot](NifiSS001.png)\r\n\r\nThe NiFi canvas will show a pre-configured main process group called \"Clinical Ingestion,\" which is the entry point to the \r\nIngestion Pattern's NiFi components. From here you can add, remove or modify ingestion processing elements, add new inputs \r\nor outputs, change the URLs to some of the other services, etc.\r\n\r\nThe Ingestion Pattern exposes a Kafka broker that can be used to feed clinical data into the pattern, but before pushing the \r\ndata in, let's create some synthetic clinical data to push. Synthetic patient data can be generated using the \r\n**Synthea Patient Generator**. Download Synthea and run the following command (for more information on Synthea visit their \r\n[Github page](https://github.com/synthetichealth/synthea)):\r\n\r\n   `java -jar synthea-with-dependencies.jar -p 10`\r\n\r\nThe previous command will have created FHIR bundles for 10 patients with their clinical history and their corresponding medical providers.\r\n\r\nNow that we have some test clinical data, let's ingest it. It's not necessary for you to install Kafka; if you have \r\n**Docker** installed you can run a container that has a Kafka producer as follows. From the list of services grab the \r\nexternal IP for the service called ingestion-kafka-0-external. This will be the address of your Kafka broker. With that, \r\nrun the following commands:\r\n\r\n  `docker run -it --rm bitnami/kafka:latest kafka-console-producer.bat --broker-list {kafka-external-ip}:9094 --topic ingestion`\r\n  \r\nAfter running the previous command (the first time you run it, it may take a minute to download the corresponding Docker \r\nimage), you will get the prompt for the Alvearie Clinical Ingestion Pattern Kafka topic:\r\n\r\n![Kafka Screen Shot](KafkaSS001.png)\r\n\r\nThe prompt accepts a patient per line, so on a different terminal, remove all new lines from one of your synthetic \r\npatients using the command below and paste the patient on the Kafka prompt:\r\n\r\n`tr -d '\\r?\\n' < patient.json > patient-single-line.json `\r\n\r\nIf you don't have Docker installed, you can also post the patient using the pattern's REST API by running the following command:\r\n\r\n`curl -X POST -d @patient.json http://{nifi-external-ip}:7001/fhirResource`\r\n\r\nAfter posting the patient either through Kafka or via HTTP, the patient will eventually be persisted in the FHIR server. \r\nFrom the list of services, grab the external IP for the service called ingestion-fhir. This will be the address of your \r\nFHIR server. \r\n\r\nYou can then query the list of FHIR resources using your browser or an HTTP client. For instance, for querying patients \r\nyou would do: [http://{fhir-external-ip}:80/fhir-server/api/v4/Patient?_format=json](http://{fhir-external-ip}:80/fhir-server/api/v4/Patient?_format=json) *(default credentials: fhiruser/integrati0n)*\r\n\r\n![FHIR Screen Shot](FhirSS001.png)\r\n\r\nFinally, from the list of services grab the external IP for the service called ingestion-grafana. \r\nThis will be the address of your Grafana server. With it, navigate \r\nto [http://{grafana-external-ip}](http://{grafana-external-ip}) *(default credentials admin/admin)*.\r\n\r\n![Grafana Screen Shot](GrafanaSS001.png)\r\n\r\n### Conclusion\r\n\r\nThe Alvearie Clinical Ingestion Pattern is not currently meant to be ready for production use out of the box, it is more of an \r\nimplementation of a Reference Design, that can evolve into a reference implementation for production use. Still, an effective \r\npattern for efficient clinical data ingestion enables advanced analytics and opens the door to a number of important \r\nhealthcare use cases, including:\r\n  - Evaluating and improving the quality of care in patient populations\r\n  - Analyzing quality measures against federal regulations and guidelines\r\n  - Accelerating time-to-insight from data that supports clinical decision-making, pharmaceutical research, and more\r\n  \r\nThe Alvearie Clinical Ingestion Pattern is fully open source and it's built using open source technology, including the \r\ncorresponding Helm charts needed to put it together and deploy it. Each of the subcomponents that comprise the pattern \r\ncan be modified using the corresponding Helm deployment properties in order to meet the persistence, availability, scalability \r\nand security requirements of a production grade deployment. \r\n\r\n\r\n<p/>\r\n<p/>\r\n<p/>\r\n","type":"Mdx","contentDigest":"f2e5bdf59459cc2aecf37afd606ca62e","counter":152,"owner":"gatsby-plugin-mdx"},"frontmatter":{"title":"Using Open Source to Build a Healthcare Record Ingestion Pattern","description":"Clinical Ingestion Blog"},"exports":{},"rawBody":"---\r\ntitle: Using Open Source to Build a Healthcare Record Ingestion Pattern\r\ndescription: Clinical Ingestion Blog\r\n---\r\nBy Luis A. Garcia &nbsp;&nbsp; | &nbsp;&nbsp; Published February 9, 2021\r\n\r\n### Introduction\r\n\r\nThe increasing digitization of healthcare records has made it more important to have a set of technologies and infrastructure \r\nthat enable healthcare organizations to effectively create, store, transform, exchange, and consume these records. This post \r\ndemonstrates how it is possible to build a reference implementation for processing healthcare records that addresses several \r\ncommon use cases, using only open source technologies.\r\n\r\nThis post introduces the Alvearie Clinical Ingestion Pattern, a reference implementation to ingest, process and store clinical records \r\nusing existing healthcare record standards. The framework fulfills several design considerations that will be outlined in the \r\nnext section and that can be extended to implement typical healthcare record use cases. It is built using open source technologies \r\nand a Helm Chart is provided in order to facilitate its deployment on a Kubernetes cluster, running potentially on any public or \r\nprivate cloud.\r\n\r\n### Design Considerations\r\n\r\nAs healthcare records have become more digitized, there have been several steps made in the right direction to facilitate \r\nworking with those records and some fundamental problems have already been solved. For example, the previously disparate \r\nset of data representation formats used by the various healthcare organizations made it harder to interoperate, however \r\nmore organizations agree on a standard way to represent and exchange these healthcare records as a result of [Health \r\nLevel Seven International (HL7)](https://www.hl7.org) creating the [Fast Healthcare Interoperability Resources (FHIR)](https://www.hl7.org/implement/standards/product_brief.cfm?product_id=491)\r\n specification. While agreeing on a standard for exchanging data is crucial to creating a healthcare records processing \r\n pipeline, it is only step one. In this section we try to discuss and solve some of additional aspects needed in order\r\n to process healthcare records effectively.\r\n \r\nThere are several non-functional characteristics that need to be considered in order to build a true healthcare records \r\nprocessing pattern that will be able to meet the challenges of the modern healthcare organization. Those characteristics include:\r\n  -\tCloud based: It should be a native cloud application with all the benefits that entails. \r\n  -\tExtensible: It should be possible for users to extend it to accomplish additional use cases on top of the basic functionality provided by the pattern. \r\n  -\tFlexible: It should be possible for users to modify comprising elements of the pattern and replace them with elements deemed better suited for the users' purposes, i.e. a \"bring your own\" (BYO) service model. \r\n  -\tOpen: It should not create vendor lock-in on any given cloud or technology, in other words it should be a multi-cloud application.\r\n  -\tScalable: It should scale as necessary to meet the user's data performance and throughput needs.\r\n\r\nFrom a functional perspective, a healthcare records processing pattern needs to allow its users to input healthcare records of \r\nmultiple kinds using some of the more commonly used formats, e.g. FHIR v4 and HL7 v3. The mechanism to accept records should \r\nallow for high throughput and the records should be persisted for traceability purposes. Multiple input modalities may be useful, \r\nfor instance input via a messaging framework and over HTTP. The records should flow through an ingestion pipeline that would \r\nnormalize the data, validate it, potentially transform it and ultimately store it. Appropriate logging, resiliency, error \r\nreporting and metrics should be maintained throughout the process.\r\n\r\nIn general, processing healthcare records falls within the realm of what is known as data integration, which is the process of \r\ncombining data from different sources and providing users a unified view of said data. Data integration use cases may involve \r\nnon-engineering teams, therefore it is a design consideration of the pattern covered in this post to provide non-engineering \r\nteams with a simple way to extend and modify the data flows.\r\n \r\n### Clinical Data Ingestion Pattern\r\n\r\nThe entire pattern is built using open source technologies, and if needed it could be expanded in functionality and scope \r\nusing other components, proprietary or open source. The following open source technologies are used in the framework:\r\n  - [Apache NiFi](https://nifi.apache.org/) is a platform for automating and managing the flow of data between disparate systems. \r\n  - [Apache NiFi Registry](https://nifi.apache.org/registry.html) is a complementary application that provides a central location for storage and management of shared resources across one or more instances of Apache NiFi. \r\n  - [Apache Kafka](https://kafka.apache.org/) is a distributed streaming platform for publishing, subscribing, storing and processing streams of records.\r\n  - [IBM FHIR Server](https://github.com/IBM/FHIR) is a modular Java implementation of version 4 of the HL7 FHIR specification with a focus on performance and configurability.\r\n  - [Prometheus](https://prometheus.io/) is an open source monitoring and alerting tool that is widely adopted across many enterprises, that monitors targets by scraping or pulling metrics from endpoints and stores the metrics in a time series database.\r\n  - [Grafana](https://grafana.com/) is an open source tool for data visualization and monitoring. Data sources such as Prometheus can be added to Grafana for metrics collection. It includes powerful visualization capabilities for graphs, tables, and heatmaps. \r\n\r\nThe foundation of the pattern is **Apache NiFi**. Apache NiFi provides a graphical user interface in the form of a canvas for data \r\nintegrators to build data processing workflows using what are known as \"processors\". NiFi comes out of the box with multiple \r\nprocessors that allow for data input and output to and from various sources, and for transforming the data into multiple data \r\nformats. Processors are linked together in such a way that the output of one becomes the input to the other, and it is in this \r\nmanner that processor groups of increasing complexity can be built.\r\n\r\nOnce a sufficiently complex and functional group of processors that achieves a specific purpose is built, it can typically be \r\nmanaged independently. Think of it as how a class, a method or a function can be abstracted out in code and managed independently \r\nas a utility. These modular process groups in NiFi are stored and managed in a **NiFi Registry**. The NiFi Registry allows for \r\nversion control and sharing of NiFi process groups, and it is backed by a Github repository where a team can be continuously \r\ndelivering process group updates or new process groups to be consumed by a NiFi server user. The NiFi Registry is the second \r\nelement of this pattern, and it is used to deliver the various NiFi processors used by the ingestion pattern. You can think \r\nof the NiFi canvas and NiFi Registry respectively as a big user-friendly empty LEGO&copy; board and a bag of working LEGO&copy; pieces \r\nthat you will use to build structures on your board.\r\n\r\nNow, in order to get data flowing in and out of the data pipeline we need a mechanism that allows for processing of the \r\ndata flowing through the pipeline in a way that is: \r\n\r\n  - Continuous, such that users don't need to wait for the pipeline to finish processing some previous data before they can submit new data \r\n  - Asynchronous, such that users don't need to wait synchronously for data to be fully processed since processing of data will likely take time\r\n  - Reliable, such that users know that once the data has been accepted it will be processed and not lost\r\n  - Durable, such that users can keep track of the raw data they have submitted into the pipeline for traceability purposes as long as necessary. \r\n\r\n**Apache Kafka** is an open source events stream framework that addresses these characteristics. The ingestion pattern connects to \r\na Kafka topic, which is just a message queue, listening for clinical data posted there, so that when data appears, it gets \r\npicked up by the framework and ingested. While posting to a Kafka topic is the main data input method, the Ingestion \r\nPattern also accepts data via a conventional REST API.\r\n\r\nClinical data that is posted to the Ingestion Pattern typically needs to be processed in various ways. Processing may include:\r\n  - Data Validation: Ensuring that the clinical data being ingested adheres to some conventions, standards or schemas. For instance, ensure the patient clinical data being ingested adheres to the FHIR US Core Profile.\r\n  - Data Transformation: Transforming the clinical data into a different format or fixing any validation errors by ensuring the failing fields adhere to the expected schemas. For instance, transforming clinical data in HL7 v3 format to FHIR, or adjusting a field to match some desired FHIR profile.\r\n  - Data Enrichment: Processing the incoming clinical data with the purpose of enriching it with more data, or metadata, that will be useful for some consumers. For instance, running some natural language processor over patient clinical notes to try to discover new clinical information.\r\n  - Data De-Identification: A very specific form of data transformation that removes any personally identifiable elements from clinical records.\r\n\r\nThe Ingestion Pattern includes pre-defined spots where one or more of these processing steps can be plugged in. It also includes \r\na configuration mechanism to specify, on a per record basis, exactly which operations from the available processing steps to \r\nrun. The pattern includes default implementations for some of these processors, but those can be replaced or complemented with \r\nothers that can be plugged in to satisfy different user needs.\r\n\r\nAfter the clinical data has been processed, it may need to be persisted. The standard for persisting clinical data is using a \r\nFHIR server. The Ingestion Pattern includes the **IBM FHIR Server** setup out of the box, which serves as the default target for \r\ningested data. That FHIR server is exposed outside the pattern where clinical data consumers can access it using the FHIR Server \r\nREST APIs.\r\n\r\nThe Ingestion Pattern includes some default activity monitoring for records that have been put through via **Prometheus** monitors \r\nover its existing components, as well as processor and memory usage of some of its components. It also includes monitors over the \r\nactual Kubernetes cluster where the pattern is running. All monitoring information can be visualized using the included **Grafana** \r\ninstance.\r\n \r\n### Installation\r\n\r\nThe instructions here assume that you have a working Kubernetes Cluster 1.10+ with Helm 3.0+.\r\n\r\nThe multiple components of the Ingestion Pattern can be deployed in a single step on a **Kubernetes** Cluster using the\r\n Alvearie Ingestion **Helm** chart. The chart performs all the steps necessary to deploy the Ingestion Pattern, ensure that \r\n there is connectivity between its various elements, set up the NiFi canvas with the corresponding processors, and initialize \r\n the necessary components. This greatly simplifies the startup process.\r\n \r\nThe following simple steps are necessary in order to run the Alvearie Clinical Ingestion Pattern:\r\n1.\tCheck out the code<br/>\r\n\t`git clone https://github.com/Alvearie/health-patterns.git`<br/>\r\n\t`cd clinical-ingestion/helm-charts/alvearie-ingestion`\r\n\t\r\n2.\tOptionally create a new namespace in your Kubernetes Cluster. <br/>\r\n\tIt is recommended, though not required, that you create a namespace before installing the chart in order to prevent the various \r\n\tartifacts it will install from mixing with the rest of the artifacts in your Kubernetes cluster, in an effort to make management \r\n\teasier.<br/>\r\n\t`kubectl create namespace alvearie`<br/>\r\n\t`kubectl config set-context --current --namespace=alvearie`\r\n3.\tInstall the helm chart with a release name ingestion:<br/>\r\n\t`helm install ingestion .`\r\n\r\nAfter running the commands above, you will see that all the corresponding elements of the Ingestion Pattern will start to be \r\nlaid out, and eventually all the Kubernetes resources will be up and running:\r\n \r\n![Screen Shot](BlogSS001.png)\r\n\r\nArchitecturally, that helm install has deployed the following:\r\n\r\n![Ingestion Architecture](BlogArch.png)\r\n \r\n### Using the Ingestion Pattern\r\n \r\nBy default, there are four external services exposed by the Alvearie Clinical Ingestion Pattern: \r\nNiFi, Kafka, FHIR, and Grafana. Let's go through them one by one along with their corresponding functionality. \r\n\r\nTo obtain the external IP for each of the exposed services run the following command:\r\n\r\n   `kubectl get services | grep LoadBalancer`\r\n\r\nNote the result of this command also includes the corresponding ports where the services are available, but the default \r\nports are also specified here for simplicity.\r\n\r\nLet's start with the *ingestion-nifi* service: [http://{nifi-external-ip}:8080/nifi](http://{nifi-external-ip}:8080/nifi)\r\n\r\n![Nifi Screen Shot](NifiSS001.png)\r\n\r\nThe NiFi canvas will show a pre-configured main process group called \"Clinical Ingestion,\" which is the entry point to the \r\nIngestion Pattern's NiFi components. From here you can add, remove or modify ingestion processing elements, add new inputs \r\nor outputs, change the URLs to some of the other services, etc.\r\n\r\nThe Ingestion Pattern exposes a Kafka broker that can be used to feed clinical data into the pattern, but before pushing the \r\ndata in, let's create some synthetic clinical data to push. Synthetic patient data can be generated using the \r\n**Synthea Patient Generator**. Download Synthea and run the following command (for more information on Synthea visit their \r\n[Github page](https://github.com/synthetichealth/synthea)):\r\n\r\n   `java -jar synthea-with-dependencies.jar -p 10`\r\n\r\nThe previous command will have created FHIR bundles for 10 patients with their clinical history and their corresponding medical providers.\r\n\r\nNow that we have some test clinical data, let's ingest it. It's not necessary for you to install Kafka; if you have \r\n**Docker** installed you can run a container that has a Kafka producer as follows. From the list of services grab the \r\nexternal IP for the service called ingestion-kafka-0-external. This will be the address of your Kafka broker. With that, \r\nrun the following commands:\r\n\r\n  `docker run -it --rm bitnami/kafka:latest kafka-console-producer.bat --broker-list {kafka-external-ip}:9094 --topic ingestion`\r\n  \r\nAfter running the previous command (the first time you run it, it may take a minute to download the corresponding Docker \r\nimage), you will get the prompt for the Alvearie Clinical Ingestion Pattern Kafka topic:\r\n\r\n![Kafka Screen Shot](KafkaSS001.png)\r\n\r\nThe prompt accepts a patient per line, so on a different terminal, remove all new lines from one of your synthetic \r\npatients using the command below and paste the patient on the Kafka prompt:\r\n\r\n`tr -d '\\r?\\n' < patient.json > patient-single-line.json `\r\n\r\nIf you don't have Docker installed, you can also post the patient using the pattern's REST API by running the following command:\r\n\r\n`curl -X POST -d @patient.json http://{nifi-external-ip}:7001/fhirResource`\r\n\r\nAfter posting the patient either through Kafka or via HTTP, the patient will eventually be persisted in the FHIR server. \r\nFrom the list of services, grab the external IP for the service called ingestion-fhir. This will be the address of your \r\nFHIR server. \r\n\r\nYou can then query the list of FHIR resources using your browser or an HTTP client. For instance, for querying patients \r\nyou would do: [http://{fhir-external-ip}:80/fhir-server/api/v4/Patient?_format=json](http://{fhir-external-ip}:80/fhir-server/api/v4/Patient?_format=json) *(default credentials: fhiruser/integrati0n)*\r\n\r\n![FHIR Screen Shot](FhirSS001.png)\r\n\r\nFinally, from the list of services grab the external IP for the service called ingestion-grafana. \r\nThis will be the address of your Grafana server. With it, navigate \r\nto [http://{grafana-external-ip}](http://{grafana-external-ip}) *(default credentials admin/admin)*.\r\n\r\n![Grafana Screen Shot](GrafanaSS001.png)\r\n\r\n### Conclusion\r\n\r\nThe Alvearie Clinical Ingestion Pattern is not currently meant to be ready for production use out of the box, it is more of an \r\nimplementation of a Reference Design, that can evolve into a reference implementation for production use. Still, an effective \r\npattern for efficient clinical data ingestion enables advanced analytics and opens the door to a number of important \r\nhealthcare use cases, including:\r\n  - Evaluating and improving the quality of care in patient populations\r\n  - Analyzing quality measures against federal regulations and guidelines\r\n  - Accelerating time-to-insight from data that supports clinical decision-making, pharmaceutical research, and more\r\n  \r\nThe Alvearie Clinical Ingestion Pattern is fully open source and it's built using open source technology, including the \r\ncorresponding Helm charts needed to put it together and deploy it. Each of the subcomponents that comprise the pattern \r\ncan be modified using the corresponding Helm deployment properties in order to meet the persistence, availability, scalability \r\nand security requirements of a production grade deployment. \r\n\r\n\r\n<p/>\r\n<p/>\r\n<p/>\r\n","fileAbsolutePath":"C:/Alvearie/alvearie.github.io/src/pages/blog/clinical-records-ingestion-pattern/index - backup.mdx"}}},"staticQueryHashes":["1054721580","1054721580","1364590287","2102389209","2102389209","2456312558","2746626797","2746626797","3037994772","3037994772","768070550","768070550"]}